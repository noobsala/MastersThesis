%======================================================================
\chapter{Up-Scaling Methodology}
%======================================================================

The goal of the following up-scaling methodology is to identify the parameters of a continuum constitutive model (macroscale model) that best emulates the average response in the DEM REV (microscale model). Let the displacement, strain and stress of the DEM REV (microscale) model be denoted by $\mathbf{u}^m$, $\boldsymbol{\epsilon}^m$, and $\boldsymbol{\sigma}^m$, respectively. Let the homogenized (averaged) strain and stress in the DEM REV model be denoted by  $\left<\boldsymbol{\epsilon}\right>$ and $\left<\boldsymbol{\sigma}\right>$, respectively.  Finally, let the strain and stress from the continuum (macroscale) constitutive model be denoted as $\boldsymbol{\epsilon}^M$ and $\boldsymbol{\sigma}^M$, respectively. The rate of macroscale stress, $\dot{\boldsymbol{\sigma}}^M=\dot{\boldsymbol{\sigma}}^M\left(\dot{\boldsymbol{\epsilon}}^M, \boldsymbol{\chi},\mathbf{h}\right)$, is defined in terms of the rate of macroscale strain, $\dot{\boldsymbol{\epsilon}}^M$, a set of material parameters $\boldsymbol{\chi}$ and a set of internal history variables $\mathbf{h}$.

The up-scaling methodology has five steps: 
\begin{enumerate}
    \item Identify the DEM REV for the NFR.
	\item Exercise the DEM REV using multiple load paths. Store $\mathbf{u}^m$, $\boldsymbol{\epsilon}^m$, and $\boldsymbol{\sigma}^m$ for each load path.
	\item Apply homogenization algorithms to the microscale results ($\mathbf{u}^m$, $\boldsymbol{\epsilon}^m$, and $\boldsymbol{\sigma}^m$) to determine the average stress-strain response of the REV, i.e., $\left<\boldsymbol{\sigma}\right>$-$\left<\boldsymbol{\epsilon}\right>$, for each load path.
	\item Identify a continuum constitutive model, $\dot{\boldsymbol{\sigma}}^M=\dot{\boldsymbol{\sigma}}^M\left(\dot{\boldsymbol{\epsilon}}^M, \boldsymbol{\chi},\mathbf{h}\right)$, that captures the salient features of NFR mechanics.
	\item Run parameter estimation algorithms to identify the parameters, $\boldsymbol{\chi}$, that minimize the difference between $\left<\boldsymbol{\sigma}\right>$-$\left<\boldsymbol{\epsilon}\right>$ and $\boldsymbol{\sigma}^M$-$\boldsymbol{\epsilon}^M$ over all load paths.
\end{enumerate}

Once an optimal parameter set, $\boldsymbol{\chi}$, for the desired model, $\dot{\boldsymbol{\sigma}}^M=\dot{\boldsymbol{\sigma}}^M\left(\dot{\boldsymbol{\epsilon}}^M, \boldsymbol{\chi},\mathbf{h}\right)$, has been identified, the newly established constitutive model can be used in Finite Element Method (FEM) models or with other suitable numerical or analytical simulations.

%----------------------------------------------------------------------
\section{Up-Scaling Implementation Overview}
%----------------------------------------------------------------------

The up-scaling framework that is presented here consists of four main software components (Figure \ref{fig:workflow}): a DEM simulator, a homogenization module, a FEM simulator, and a parameter estimation module. In procedural order, the first software component involved is a DEM simulation package, which is used to directly model the NFR. 

\begin{figure}[p]
\begin{center}
\includegraphics[width=0.75\textwidth]{figures/Chapter3/UpScalingFlowChart}
\caption{{\label{fig:workflow}Up-scaling workflow used to estimate the optimal continuum model parameter set. The DEM software (a) produces a data set which is run through homogenization software (b) which in turn produces another dataset that is fed into the parameter estimation program (c). This parameter estimation program drives the CDM simulations (d) iteratively in order to find an optimal parameter set for the fitted model.%
}}
\end{center}
\end{figure}

The DEM software accepts as inputs the geometry of the DFN, the material properties of the rock and the natural fractures, and the load paths. The DEM REV is exercised for different load-paths in a way that is akin to conducting multiple triaxial tests on physical specimens to characterize the full range of material behaviour. The DEM software outputs the microscale displacement, $\mathbf{u}^m$, and stress-strain, $\boldsymbol{\sigma}^m$-$\boldsymbol{\epsilon}^m$, responses for each load path.  This microscale data is subsequently fed into the homogenization module to compute the average stress-strain response, $\left<\boldsymbol{\sigma}\right>$-$\left<\boldsymbol{\epsilon}\right>$,  for each load path. Next, the homogenized stress-strain data, $\left<\boldsymbol{\sigma}\right>$-$\left<\boldsymbol{\epsilon}\right>$, is used by the parameter estimation software as observation data (i.e., laboratory/field data). The parameter estimation module iteratively executes a constitutive model, $\dot{\boldsymbol{\sigma}}^M=\dot{\boldsymbol{\sigma}}^M\left(\dot{\boldsymbol{\epsilon}}^M, \boldsymbol{\chi},\mathbf{h}\right)$, embedded in the FEM simulator for each load path using different parameter sets, $\boldsymbol{\chi}^i$, while attempting to minimize the error between the homogenized microscale, $\left<\boldsymbol{\sigma}\right>$-$\left<\boldsymbol{\epsilon}\right>$, and macroscale, $\boldsymbol{\sigma}^M$-$\boldsymbol{\epsilon}^M$,  stress-strain curves. Eventually, the algorithm converges to a near-optimal parameter set, $\boldsymbol{\chi}$, that can be viewed to be the best estimate of the NFR responses by the given continuum model. 

In our implementation, UDEC\textsuperscript{TM} was used as the DEM simulator and ABAQUS\textsuperscript{TM} was used as the FEM simulator. In ABAQUS\textsuperscript{TM}, single element simulations were performed with a strain-history prescribed through displacement boundary conditions for a given set of material parameters and the stress is obtained as the output. There is nothing particularly special about the DEM or FEM simulators chosen; each could easily be replaced to overcome any inherent limitations.  Moreover, a FEM simulator is not actually needed, since its inclusion in this framework is simply to gain access to the constitutive models within. The FEM simulator could easily be replaced by a FDM simulator or simply by a material subroutine.  OSTRICH\textsuperscript{TM}, a model-independent optimization package \citep{matott_ostrich:_2016}, is used for the parameter estimation module. The homogenization module was written in-house in Python\textsuperscript{TM}. Python\textsuperscript{TM} was also used to interface and drive the various components of the up-scaling framework.

%----------------------------------------------------------------------
\section{Distinct Element Method}
%----------------------------------------------------------------------

Discontinuous systems are characterized by the existence of discontinuities that separate discrete domains within the system. In order to effectively model a discontinuous system, it is necessary to represent two distinct types of mechanical behaviour: the behaviour of the discontinuities and the behaviour of the solid material.

There exists a set of methods, referred to as discrete element methods, which provide the capacity to explicitly represent the behaviour of multiple intersecting discontinuities. The methods allow for the modelling of finite displacements and rotations of discrete bodies, including contact detachment as well as automatic detection of new contacts. Within the set of Discrete Element Methods, there are four subsets \citep{CUNDALL_1992}: Modal Methods, Discontinuous Deformation Analysis Methods, Momentum Exchange Methods, and Distinct Element Methods (DEM). In this paper, the discrete element method used is DEM.

With DEM methods, the discontinuous system is represented as an assembly of deformable blocks such that the interfaces between the blocks represent the discontinuities. With respect to NFR, the blocks can be used to represent the intact rock while the discontinuities represent the joints in the rock mass. 

Consider an arbitrary deformable domain, $\Omega$, with a boundary, $\Gamma$, that is subdivided by prescribed discontinuities into $i$ number of subdomains, each denoted by $\Omega_i$ (Figure \ref{fig:DEM}). Let $\Gamma_{ij}=\Gamma_{ji}$ represent the boundary between $\Omega_i$ and $\Omega_j$. The motion of these subdomains (discrete elements) is governed by the conservation of momentum which relates the divergence of the stress field within the element, $\nabla\cdot\boldsymbol{\sigma}^m$, to the element acceleration, $\ddot{\mathbf{u}}^m$, and density, $\rho$:

\begin{equation}
\rho \ddot{\mathbf{u}}^m=\nabla\cdot\boldsymbol{\sigma}^m
\label{eqn:cauchy}
\end{equation}

\begin{figure}[!htb]
\begin{center}
\includegraphics[width=0.7\textwidth]{figures/Chapter3/DEMSchematic}
\caption{{\label{fig:DEM}DEM block formulation for an arbitrary domain, $\Omega$, with a set of subdomains, $\Omega_i$.%
}}
\end{center}
\end{figure}

The interaction between $\Omega_i$ and $\Omega_j$ along $\Gamma_{ij}$ is the distinguishing feature in the DEM formulation, and is comprised of two main components: contact detection and the constitutive relationship. The contact detection algorithms are responsible for ensuring that $\Omega_i$ and $\Omega_j$ do not penetrate each other and ensuring that the appropriate contact forces are transferred between elements. These contact forces are governed by constitutive models of $\Gamma_{ij}$ which can be described in general by a shear stiffness, $k_s$, in a direction parallel to the $\Gamma_{ij}$, and a normal stiffness, $k_n$, in a direction normal to $\Gamma_i$. The normal stress over any $\Gamma_{ij}$, $\sigma^n$, can be expressed as a function of the normal elastic displacement, $u^n$, up until the tensile strength, $T$, is exceeded: 

\begin{equation}
\sigma^n=\left\{\begin{matrix}
\sigma^n\left(k^n, u^n\right) &if& \sigma^n \geq -T\\ 
 0 & if &\sigma^n < -T
\end{matrix}\right.
\label{eqn:demnormal}
\end{equation}

Futhermore, the shear stress, $\tau$, over any $\Gamma_{ij}$ can be written in terms of the elastic shear displacement, $u^s$, until the maximum shear strength, $\tau^{max}$ is reached. The point when the shear stress at a point on $\Gamma_{ij}$ exceeds the prescribed maximum shear stress, the discontinuity experiences plastic shear displacements in order not to exceed the maximum shear stress:

\begin{equation}
\tau=\left\{\begin{matrix}
\tau\left(k^s,u^s, \sigma^n\right) &if&\left |\tau \right | < \tau^{max}\\ 
\frac{u^s}{\left|u^s\right|}\tau^{max} & if &\left |\tau \right | \geq \tau^{max}
\end{matrix}\right.
\label{eqn:demshear}
\end{equation}

The microscale stress field, $\boldsymbol{\sigma}^m$, within $\Omega_i$ is described by standard continuum constitutive relationships. With the constitutive behaviour of the discontinuities and the constitutive behaviour of the continuum blocks, the overall behaviour of the rock mass can be characterized through material properties of the rock discontinuities and the intact rock.

%----------------------------------------------------------------------
\section{Homogenization Approach}
%----------------------------------------------------------------------

The main objective of up-scaling DEM simulations is to be able to describe the  behavior of the discontinuous medium in terms of a more computationally efficient continuum model. The homogenization algorithms used herein to determine the average stress-strain behaviour, $\left<\boldsymbol{\sigma}\right>$-$\left<\boldsymbol{\epsilon}\right>$, of the REV from the microscale displacements $\mathbf{u}^m$, strain $\boldsymbol{\epsilon}^m$, and stresses $\boldsymbol{\sigma}^m$ are based on the methods developed by \citet{daddetta_particle_2004} and \citet{wellmann_homogenization_2008}. In this homogenization process, the resultant inter-block contact forces and block displacement from the DEM simulations are converted to average stresses and strains.

For the homogenization procedure to yield meaningful results, it should be applied to a Representative Elementary Volume (REV). The exact size of the REV depends on the geometry and mechanical properties of the DEM model. For the homogenization approach to hold, the REV of size $d$ within a system with a characteristic length $D$ and consisting of blocks with a characteristic diameter $\delta$, must satisfy scale separation \citep{wellmann_homogenization_2008}:
 
\begin{equation}
\label{eqn:scalesep}
D\gg d\gg\delta
\end{equation}

In the following sections, all deformations are assumed to be small, such that there is no need to differentiate between the deformed and undeformed configurations.  

We begin by defining a $L\times L$ square DEM simulation domain over which mixed-boundary conditions will be applied. The REV is taken as a circular domain of radius $R$, $2R<L$.  The REV is taken to be a subdomain of the actual DEM simulation domain to eliminate any boundary effects.  As will be seen below, it is convenient to take the boundary of the domain used for homogenization as a slightly larger domain encompassing the REV boundary. The boundary of the homogenization domain, denoted as $\Gamma_h$, is defined by the outer edges of the deformable blocks, i.e., the cohesive/contact surfaces between deformable blocks, which intersect a circle of radius $R$ located in the center of the DEM simulation domain. Let the homogenization domain, the domain bounded by $\Gamma_h$, be denoted by $\Omega_h$. These definitions are illustrated in Figure \ref{fig:homoarea} for a $10$m $\times$ $10$m DEM domain, where the deformable blocks are defined through a Voronoi tessellation. The radius of the REV domain is $2.5$m.  It can be seen that the actual domain used for homogenization is non-circular and larger than the REV domain. 

\begin{figure}[!htb]
\begin{center}
\includegraphics[width=\textwidth]{figures/Chapter3/HomogenizationArea}
\caption{{\label{fig:homoarea}Assessment of the homogenization boundary given a circular REV.%
}}
\end{center}
\end{figure}

Given a circular REV, due to the discontinuous nature of the DEM simulations, the circular REV cannot be used directly. Because the calculated displacements and contact forces from the DEM are known at the block edges, the homogenization domain boundary must follow the block boundaries. The homogenization domain is characterized by a series of block corners which are identified at the initial (zero strain) state. These corners continue to define the homogenization domain once deformation occurs, allowing for a consistent homogenization domain definition as the model is deformed. An algorithm is developed here to assess the homogenization domain boundary: 

\begin{enumerate}
\item \label{hli:1} Identify all blocks that interect the circular REV boundary. 
\item \label{hli:2} Identify all blocks that lie completely outside the REV boundary. 
\item \label{hli:3} Find all contacts corresponding to the intersection between blocks in \ref{hli:1} and \ref{hli:2}. 
\item \label{hli:4} Find all corners corresponding to the contacts in \ref{hli:3}. 
\item \label{hli:5} Find all contacts of blocks in \ref{hli:1}. 
\item \label{hli:6} Find all blocks corresponding to the intersection of contacts in \ref{hli:3} and \ref{hli:5}. 
\item \label{hli:7} Find all corners corresponding to blocks in \ref{hli:6}. 
\item \label{hli:8} Find intersection of corners in \ref{hli:4} and \ref{hli:7}. 
\item \label{hli:9} Find corners that initially (at zero strain) coincide with the corners in \ref{hli:8} 
\item \label{hli:10} Find union of corners in \ref{hli:8} and \ref{hli:9}. 
\item \label{hli:11} Order corners from \ref{hli:10} such that they form the closed boundary of the homogenization area.
\end{enumerate}

This algorithm for defining the homogenization domain is developed to provide an unambiguous method for assessing a unique homogenization domain for a given circular REV at a specific location. In steps \ref{hli:1} and \ref{hli:2}, two mutually exclusive block sets are identified, which necessarily share contacts. It is the boundary between these two block sets that is the homogenization domain boundary which is characterized by this algorithm. 

Ultimately, the goal of this process is to identify the corners on this boundary that are on the blocks that intersect with the REV boundary and not the blocks that are outside the REV boundary. As such, the corners (step \ref{hli:4}) corresponding to the contacts (step \ref{hli:3}) between the two sets of blocks are determined. Step \ref{hli:5} is necessary to help eliminate some blocks from the set of boundary block which intersect the REV boundary, but only have contact with blocks inside the homogenization domain boundary, and thus do not have any overall contribution to the definition. Step \ref{hli:6} determines this resultant set of boundary blocks of which every member is connected to the boundary in some capacity. Finding the set of corners that is mutually shared by these blocks in step \ref{hli:7} and the boundary contact corners in step \ref{hli:4} allow for the determination of the initial set of boundary corners (step \ref{hli:8}).

However, one must also note the potential displacement jumps that may occur between blocks on the boundary as the model deforms. In the case where the blocks become physically separated, there exists a discontinuity along the homogenization boundary. These discontinuities along the homogenization boundary are considered by adding boundary segments to the homogenization boundary between the corners of the adjacent blocks. As such, steps \ref{hli:9} and \ref{hli:10} find coincident corners on adjacent boundary blocks to allow for the blocks to become separated on the boundary, while still maintaining connectivity along the homogenization domain boundary. It is also necessary to order the corners (step \ref{hli:11}) to define a closed set of boundary segments along which integration can be performed.

The homogenization boundary, $\Gamma_{h}$, can be described in terms of $n$ ordered boundary vertices, $V_{i}^{h}=(x_{i}^{h},y_{i}^{h})$, representing the $i$-th set of vertex coordinates along the boundary, such that the area of the homogenization domain, $A^{h}$, can be calculated using the following formulation for the area of an arbitrary, non-self-intersecting polygon \citep{Zwillinger_1995}: 

\begin{equation}
A^{h}=\dfrac{1}{2}\sum_{i=1}^{n}x_{i}^{h}(y_{i+1}^{h}-y_{i-1}^{h})
\label{eqn:hom1}
\end{equation}

\subsection{Stress Homogenization}

The homogenized Cauchy stress, $\langle\boldsymbol{\sigma}\rangle$, is derived from the definition of the spatial average of the microscale stress of the deformable blocks of the DEM simulation, $\boldsymbol{\sigma}^m$, over the homogenization domain $\Omega^{h}$. 
\begin{equation}
\langle\boldsymbol{\sigma}\rangle=\frac{1}{A^{h}}\int_{\Omega^{h}}\boldsymbol{\sigma}^m dA\label{eqn:stress1}
\end{equation}

Since the block subdomain is inherently discretized, the integration of the stress over the homogenization domain in can be written as a summation of the average stress in each block. Let $\boldsymbol{\sigma}_{I}^{m}$ denote the average stress in block $I$ with an area $A_{I}$ and let $N^{b}$ represent the number of blocks in the homognenization domain. The homogenized stress from (\ref{eqn:stress1}) can now be written as:

\begin{equation}
\langle\boldsymbol{\sigma}\rangle=\frac{1}{A^{h}}\sum_{I=1}^{N^{b}}\boldsymbol{\sigma}_{I}^{m}A_{I}\label{eqn:stress2}
\end{equation}

When each deformable block is discretized by constant stress triangles (elements/zones), the integration of the stress over each deformable block subdomain can be written as a summation in the form of a spatially weighted average of the zone stresses. Let $\boldsymbol{\sigma}_{IJ}^{m}$ denote the stress in zone $J$ of deformable block $I$, $N_{I}^{z}$ denote the number of zones within block $I$, and $A_{IJ}$ denote area of zone $J$ in block $I$. The average stress in a block is then defined as:

\begin{equation}
\boldsymbol{\sigma}_{I}^{m}=\frac{1}{A_{I}}\sum_{J=1}^{N_{I}^{z}}\boldsymbol{\sigma}_{IJ}^{m}A_{IJ}\label{eqn:stress3}
\end{equation}

Here, substituting (\ref{eqn:stress3}) into (\ref{eqn:stress2}) allows us to write the homogenized stress in terms of the zone stresses:

\begin{equation}
\langle\boldsymbol{\sigma}\rangle=\frac{1}{A^{h}}\sum_{I=1}^{N^{b}}\sum_{J=1}^{N_{I}^{z}}\boldsymbol{\sigma}_{IJ}^{m}A_{IJ}\label{eqn:stress4}
\end{equation}

In our implementation, (\ref{eqn:stress4}) is incorporated into the homogenization module.

\subsection{Strain Homogenization}

The derivation for the homogenized strain tensor, $\left< \boldsymbol{\epsilon}\right>$, begins in a similar manner to the homogenized stress tensor derivation with the familiar definition of the spatial average:

\begin{equation}
\langle\boldsymbol{\epsilon}\rangle=\frac{1}{A^{h}}\int_{\Omega^{h}}\boldsymbol{\epsilon}^m dA\label{eqn:strain2}
\end{equation}

At this point, it becomes convenient to assume a small displacement formulation of strain. This displacement assumption limits the applicability of the strain homogenization, but in the context of large scale geomechanics, this assumption remains reasonable. As such, the linear infinitesimal strain tensor can be written in terms of the displacement vector, $\mathbf{u}^m$:

\begin{equation}
\boldsymbol{\epsilon}^m=\frac{1}{2}\left[\nabla\mathbf{u}^m+\left(\nabla^\top \mathbf{u}^m\right)\right]\label{eqn:strain1}
\end{equation}

The above integral can be converted to the following boundary integral using the divergence theorem where $\mathbf{n}$ is the outward pointing normal to $\Gamma_h$. :

\begin{equation}
\langle\boldsymbol{\epsilon}\rangle=\frac{1}{2A^{h}}\oint_{\Gamma^{h}}\left[\mathbf{u}^m\otimes\mathbf{n}+\mathbf{n}\otimes\mathbf{u}^m\right]d\Gamma\label{eqn:strain5-1}
\end{equation}

When $\Gamma_h$ is defined by a set of line segments over which the displacement is also linear, the boundary integral can be rewritten as a summation over each of the $N$ boundary segments. Let $\bar{\mathbf{u}}^m_{I}$ denote the average displacement along the $I^{th}$ boundary segment of the homogenization boundary, which is calculated as the average of the two nodal displacements defining the boundary of each segment. Let $\mathbf{n}_{I}$ represent the outward pointing normal to the $I^{th}$ boundary segment on the homogenization boundary.  Let the length of boundary segment $I$ be denoted by $L_{I}$. The homogenized strain can be rewritten as

\begin{equation}
\langle\boldsymbol{\epsilon}\rangle=\frac{1}{2A^{h}}\sum_{I=1}^{N}\left[\bar{\mathbf{u}}^m_{I}\otimes\mathbf{n}_{I}+\mathbf{n}_{I}\otimes\bar{\mathbf{u}}^m_{I}\right]L_{I}\label{eqn:strain7}
\end{equation}

In our implementation, (\ref{eqn:strain7}) is incorporated into the homogenization module.

%----------------------------------------------------------------------
\section{Assessment of the REV Size}
%----------------------------------------------------------------------

Homogenizing DEM simulations requires the existence and determination of the REV for the given medium. Generally, the REV of a given domain can be described as the smallest subdomain that is statistically representative of the entire domain \citep{Kanit_2003, Gitman_2007}. This qualitative definition is insufficient to rigorously define an REV as it is subjective with respect to what "statistically representative" means. Hence, the assessment of the REV can be a contentious issue, fraught with ambiguity.

One can conceptualize an REV to be "statistically representative" in two primarily different ways \citep{Drugan_1996}. The classically cited means for characterizing an REV suggests that the micro-scale heterogeneities (e.g. fractures, voids, grains, etc.) should be statistically representative within the REV such that the REV should contain a sufficiently large sample of these heterogeneities. This characterization of the REV is potentially problematic when attempting to quantify the REV as the descriptions of these heterogeneities tend to be nominally qualitative, and at best, quasi-quantitative.

The alternative means of conceptualizing "statistically representative", and arguably a more pragmatic way, proposes that the constitutive response of the REV should be statistically representative of the domain. In other words, as one increases the size of a sample domain, the point at which the constitutive response within the domain becomes sensibly constant can be referred to as the REV. Thus, the subdomain constitutive response is quantifiable through resultant model properties and parameters. This REV interpretation has been widely used because of its quantifiability \citep{Kanit_2003, Gitman_2005, Gusev_1997, M_ller_2010}, and is adopted for this work.

Here, the aim of this discussion is to present tools to help evaluate the size and existence of the REV for a given microscopically heterogeneous domain. The method of assessing the REV that is presented here is based on testing the statistical significance of the sample distribution parameters. In this method of REV assessment, only one numerical simulation is required if it is of sufficient size. Instead of running the numerical simulations for each realization, subsets of the larger simulation are extracted and analysed independently for their respective constitutive response. This method is far less computationally demanding and correspondingly much faster as a result. 

For a given measurement of a material property (e.g. stress, strain, etc.) in an REV, one can assume that it is primarily variant within the heterogenous domain with respect to two key parameters: the spatial location of the subdomain and the subdomain size. As such, these parameters can be modified to observe the changes in the measured properties of the subdomain. To clarify some terminology, the set of data that corresponds to a single location will be termed a realization. Whereas the set of data that corresponds to a single subdomain size will be referred to as a sample in the statistical sense. Therefore the statistical population can be interpreted as the set of all possible material property measurments for a given subdomain size as the spatial location of the subdomain changes. 

To assess the size of the REV for a given realization, a selection of subdomain sizes is chosen and the material property measurement is evaluated over each subdomain. The material property measurement can be plotted against the corresponding subdomain size as conceptually indicated in Figure \ref{fig:revConvergence}. In this plot, one can see that as the subdomain size increases, the variability in the material property measurement decreases such that it converges to a single value. It is at this convergence that one can say that the subdomain is statistically representative of the whole domain, and therefore represents the REV for that realization. Each realization is spatially static such the spatial variability is not sampled. As such, in order to account for the spatial variability of the material property measurement, multiple realizations are required to accurately define the REV.

\begin{figure}[!htb]
\begin{center}
\includegraphics[width=\textwidth]{figures/Chapter3/REVConvergence}
\caption{{\label{fig:revConvergence}Assessing the REV of a heterogeneous domain for a single realization. For very small subdomain sizes, the material property measurement is very sensitive to small fluctuations, whereas at a sufficiently large subdomain size, the material property measurement becomes insensitive to small fluctuations and converges to a single value representative of the whole domain. It is at this convergence that one can say the REV exists.%
}}
\end{center}
\end{figure}

To state the problem more formally, consider a heterogeneous domain, $\Omega$, in which an REV exists and is subject to a uniform loading. Now assume that this domain has a measurable property, $X_i$, which will be used to define the REV. This property, for a given sample, $i$, is represented as a random variable due to the spatial variability in the sampling domain. This parameter can be assumed to follow normal distribution, $N\left(\mu,\sigma^2 \right)$, where the subscript, $i$, represents each sample of a given subdomain size:

\begin{equation}
X_i \approx N\left(\bar{x}_i,s_i \right)
\label{eqn:rev1}
\end{equation}

Given that $X_i$ is expected to be normally distributed within the sample, there exists two distribution parameters that completely describe the sample distribution: the mean, $\mu$, and the variance, $\sigma^2$. As such, the proposed method for assessing the REV relies on assessing whether the population mean, $\mu$, is statistically equivalent to the sample mean, $\bar{x}_i$, and whether the population variance, $\sigma^2$, is statistically equivalent to the sample variance, $s^2$. Because of the uniform loading, $X_i$ for the REV is expected to have the same value regardless of its location in the domain. As such, the statistical criteria to be fulfilled are such that the expected value of the mean, $E\left(\bar{x}\right)$, and the expected value of the variance, $E\left(s^2\right)$, are equivalent to the population mean and population variance:

\begin{equation}
E\left(\bar{x}\right) = \mu
\label{eqn:rev2}
\end{equation}

\begin{equation}
E\left(s^2\right) = \sigma^2
\label{eqn:rev3}
\end{equation}

In order to assess these criteria for each sample, hypothesis testing is employed. In order to test (\ref{eqn:rev2}), the independent one sample t-test is used, while to test (\ref{eqn:rev3}), the chi-squared test for variance is used. In both tests, if null hypothesis is not rejected, it can be said that the subdomain is statistically representative of the whole domain with respect to that parameter. As such, with this method, one can define the REV to be the minimum subdomain size for which both of the aforementioned null hypotheses are accepted.

\subsection{Spatial Variance of the REV}

The acceptability of the spatial variance of the sample can be tested with the chi-squared test for variance. The chi-squared test for the variance is a hypothesis testing method used to test whether or not a sample is statistically likely to have a specified population variance if the population belongs to a normal distribution \citep{walpole_probability_2007}. In this case, it is desireable to know whether or not a given samples variance is equivalent to a tolerable population variance, $\sigma^2$. As such, the null hypothesis, $H_0$, can be stated as follows:

\begin{equation}
H_0:s^2=\sigma_t^2
\label{eqn:rev4}
\end{equation}

Where in the case that the null hypothesis is rejected, the alternate hypothesis, $H_A$, for an upper one-tail test is specified to be the following

\begin{equation}
H_A:s^2>\sigma_t^2
\label{eqn:rev5}
\end{equation}

This alternate hypothesis is specified because this is the only case in which the sample variance does not fulfill the criteria for the REV (if the variance is smaller than the population variance, a REV can still be defined). As such, when the null hypothesis is rejected, the capacity for the specified subdomain size to be a REV is also rejected. To test this hypothesis, the chi-squared test statistic, $T_{\chi^2}$, is given as follows \citep{walpole_probability_2007}:

\begin{equation}
T_{\chi^2}=\left[N-1\right]\left[\frac{s}{\sigma_t}\right]^2
\label{eqn:rev6}
\end{equation}

Where $N$ is the sample size. In (\ref{eqn:rev6}), the tolerable population standard deviation is unknown. This method estimates the tolerable standard deviation of the population based on a specified tolerance, $\alpha$, and the expected value of the realizations:

\begin{equation}
\sigma=\alpha E\left(\bar{x}_i\right)
\label{eqn:rev7}
\end{equation}

Due to the fluctuations in the sample means for the samples corresponding to small subdomain sizes, the expected value of the realizations was taken to be the sample mean for the largest subdomain, $\bar{x}_i$:

\begin{equation}
E\left(\bar{x}_i\right) = \bar{x}_n
\label{eqn:rev8}
\end{equation}

As such, one can substitute (\ref{eqn:rev7}) and (\ref{eqn:rev8}) into (\ref{eqn:rev6}) to rewrite the chi-squared test statistic as follows:

\begin{equation}
T_{\chi^2}=\left[N-1\right]\left[\frac{s}{\alpha \bar{x}_i}\right]^2
\label{eqn:rev9}
\end{equation}

For the upper one-tailed chi squared test for variance, the following condition rejects the null hypothesis in favour of the alternative hypothesis where $\chi_{\left(\alpha, \nu\right)}$ is the critical chi-distribution test statistic for a significance level, $\alpha$, and $N-1$ degrees of freedom , $\nu$ \citep{walpole_probability_2007}:

\begin{equation}
T_{\chi^2}>\chi^2_{\left(1-\alpha, N-1\right)}
\label{eqn:rev10}
\end{equation}

\subsection{REV Size Convergence}

The convergence of the REV constitutive response as the REV increases in size can be tested with the independant one sample t-test. The independent one sample t-test is a hypothesis testing method used to test whether or not a sample is statistically likely to have a specified population mean when the population variance is unknown if the population belongs to a normal distirbution \citep{walpole_probability_2007}. Since the population variance is unknown, that parameter has to be estimated by means of Student’s t-distribution. In this case, it is desired to know whether or not a given samples mean is representative of the population mean. As such, the null hypothesis, $H_0$, can be stated as follows:

\begin{equation}
H_0:\bar{x}=\mu
\label{eqn:rev11}
\end{equation}

Where if the null hypothesis is rejected, the alternate hypothesis, $H_A$, for a two-tailed hypothesis test can be written as:

\begin{equation}
H_A:\bar{x}\neq\mu
\label{eqn:rev12}
\end{equation}

Such that in the case that the null hypothesis is rejected, the sample mean is not statistically representative of the population mean. The test statistic, $T_t$, for the t-test is indicated to be \citep{walpole_probability_2007}:

\begin{equation}
T_t=\left[X-\mu\right]\frac{\sqrt{N}}{s}
\label{eqn:rev13}
\end{equation}

Where $N$ is the size of the sample. The population mean, $\mu$, is prescribed in a similar manner as \ref{eqn:rev8} by assuming the expected population mean to be equivalent to the sample mean corresponding to the largest subdomain:

\begin{equation}
\mu=\bar{x}_n
\label{eqn:rev14}
\end{equation}

To evaluate the null hypothesis and determine whether or not the means are statistically equivalent, the following criterion is employed, where$t_{\left(\alpha, \nu\right)}$ is the t-distribution test statistic for a significance level, $\alpha$, and $N-1$ degrees of freedom, $\nu$ \citep{walpole_probability_2007}:

\begin{equation}
\left|T_t\right|>\left|t_{\left(1-\frac{\alpha}{2}, N-1\right)}\right|
\label{eqn:rev15}
\end{equation}

%----------------------------------------------------------------------
\section{Macroscale Constitutive Model}
%----------------------------------------------------------------------

In this section we describe two macroscale stress-strain relationships, $\dot{\boldsymbol{\sigma}}^M=\dot{\boldsymbol{\sigma}}^M\left(\dot{\boldsymbol{\epsilon}}^M, \boldsymbol{\chi},\mathbf{h}\right)$, used in the validation examples. The models are chosen to be complex enough to make the validation of the framework meaningful; however, we do not claim that these are the "best" macroscale models. The framework presented is general and the macroscale constitutive models described here can be replaced in particular applications by different ones. To simplify the discussion and notation in this section, the superscript "$M$" is omitted since all quantities defined describe macroscale behaviour.

Continuum Damage Mechanics (CDM) constitutive models are chosen to represent the NFR at the macroscale. CDM is a branch of continuum mechanics that is concerned with modeling the progressive failure and stiffness degradation in solid materials. CDM in this investigation is used to help describe the micro-mechanical degradation of the rock mass due to the nucleation and growth of cracks and voids. This micro-mechanical degradation is represented in a CDM model by using macroscopic state variables to represent a spatial average of the effects of this degradation. These state variables used in this context with respect to CDM are known as damage variables. 

The damage variables in a CDM model can be described in different capacities. Often, for mathematical and physical simplicity, a single scalar damage variable is used to characterize the state of damage in the material. In this case, the damage variable, $D$, takes a value between 0 and 1 to represent the degree of damage to the material, where $D=0$ represents a completely undamaged material (original stiffness) and $D=1$ represents a completely damaged material with no stiffness. A scalar damage description limits the applicability of the CDM model to an isotropically damaged state, which may not be appropriate in some circumstances. More sophisticated CDM models use 2\textsuperscript{nd} and 4\textsuperscript{th} order tensorial representations of the damage variables as well as distinguishing between compressive damage and tensile damage states in order to more accurately characterize anisotropic damage evolution. 

Consider the standard elastic relationship described by Hooke's law which relates the stress, $\boldsymbol{\sigma}$, and the elastic strain, $\boldsymbol{\epsilon}^{el}$ through an elastic stiffness tensor, $\mathbf{E}$:

\begin{equation}
\boldsymbol{\sigma}=\mathbf{E}:\left(\boldsymbol{\epsilon}^{el}\right)
\label{eqn:const1}
\end{equation}

Applying a scalar damage variable, $D$, to Hookes law using CDM to describe the stiffness degredation of the material can be shown as:

\begin{equation}
\boldsymbol{\sigma}=\left(1-D\right)\mathbf{E}:\left(\boldsymbol{\epsilon}^{el}\right)
\label{eqn:const2}
\end{equation}

Here, the damaged stiffness of the material, $\mathbf{E}^d$, is described as folows:

\begin{equation}
\mathbf{E}^d=\left(1-D\right)\mathbf{E}
\label{eqn:const3}
\end{equation}

Allowing the constitutive elastic CDM relationship to be written as:

\begin{equation}
\boldsymbol{\sigma}=\mathbf{E}^d:\boldsymbol{\epsilon}^{el}
\label{eqn:const4}
\end{equation}

In addition to damage, the elasto-plastic behaviour of the rock is also considered. Models that incorporate theories of plasticity and damage mechanics in a unified approach to damage evolution and constitutive relationships are often referred to as damage-plasticity models \citep{zhang_continuum_2010}. In general, the constitutive relation for these damage-plasticity models describes the relationship between the stress, $\boldsymbol{\sigma}$, and the strain, $\boldsymbol{\epsilon}$ as a function of the damage variable, the original elastic stiffness tensor, $\mathbf{E}$, and the plastic strain, $\boldsymbol{\epsilon}^{pl}$: 

\begin{equation}
\boldsymbol{\sigma}=\mathbf{E}^d:\left(\boldsymbol{\epsilon}-\boldsymbol{\epsilon}^{pl}\right)
\label{eqn:const5}
\end{equation}

Here, an additive decomposition of the elastic and plastic strain is assumed:
\begin{equation}
\boldsymbol{\epsilon}=\boldsymbol{\epsilon}^{el}+\boldsymbol{\epsilon}^{pl}
\label{eqn:const6}
\end{equation}

In CDM, the notion of effective stress, $\bar{\boldsymbol{\sigma}}$, becomes useful to describe the mechanics of the system as it refers to the stress that the system would be experiencing without damage. This effective stress can be related to the actual Cauchy stress through the scalar damage variable: 

\begin{equation}
\boldsymbol{\sigma}=\left(1-D\right)\bar{\boldsymbol{\sigma}}
\label{eqn:const7}
\end{equation}

\subsection{General Formulation and Assumptions}

The plasticity models used here are assumed to comprise of three key components: The yield function, the flow rule and the hardening rule. The yield function, $F\left(\bar{\boldsymbol{\sigma}},\bar{\epsilon}^{pl}\right)$ indicates whether or not the material has experienced yield given a particular stress state. The yield function varies between the two models but can be written in general as a function of the effective stress, $\bar{\boldsymbol{\sigma}}$, and equivalent plastic strain, $\dot{\bar{\epsilon}}^{pl}$, expressed through three stress invariants: the Von-Mises equivalent stress, $p\left(\bar{\boldsymbol{\sigma}}\right)$, the hydrostatic stress, $q\left(\bar{\boldsymbol{\sigma}}\right)$, and the third invariant of deviatoric stress, $r\left(\bar{\boldsymbol{\sigma}}\right)$:

\begin{equation}
    F = F\left(\bar{\boldsymbol{\sigma}}\right)
\label{eqn:gen1}
\end{equation}

The Von Mises equivalent stress is written as:

\begin{equation}
p\left(\bar{\boldsymbol{\sigma}}\right) = \frac{1}{3}\Tr\left[\boldsymbol{\sigma}\right]
\label{eqn:gen2}
\end{equation}

The hydrostatic stress is written as:

\begin{equation}
q\left(\bar{\boldsymbol{\sigma}}\right)=\sqrt{\frac{3}{2}}\left[\mathbf{S}:\mathbf{S}\right]
\label{eqn:gen3}
\end{equation}

Where $\mathbf{S}$ is known as the stress deviator with $\mathbf{I}$ being the identity matrix:

\begin{equation}
\mathbf{S}=\boldsymbol{\sigma}+p\left(\bar{\boldsymbol{\sigma}}\right)\mathbf{I}
\label{eqn:gen4}
\end{equation}

The third invariant of the deviatoric stress is written as:

\begin{equation}
r\left(\bar{\boldsymbol{\sigma}}\right)=\left[\frac{9}{2}\mathbf{S}\cdot\mathbf{S}:\mathbf{S}\right]^{frac{1}{3}}
\label{eqn:gen5}
\end{equation}

In addition, the flow rule describes the amount of plastic deformation that the material should exhibit given an applied stress. The flow rule in these models is assumed to be of the following form:

\begin{equation}
\dot{\boldsymbol{\epsilon}}^{pl}=\dot{\lambda} \frac{\partial G\left(\bar{\boldsymbol{\sigma}}\right)}{\partial \bar{\boldsymbol{\sigma}}}
\label{eqn:gen6}
\end{equation}

Where $\dot{\boldsymbol{\epsilon}}^{pl}$ is the plastic strain rate, $\dot{\lambda}$ is referred to as the plastic consistency parameter, and $G\left(\bar{\boldsymbol{\sigma}}\right)$ is the flow potential function. In addition to the yield function and the flow rule, the hardening rule is prescribed to govern the increase/decrease in yield stress as the plastic strain increases. More specifically, the hardening function, $\boldsymbol{h}\left(\bar{\boldsymbol{\sigma}}, \bar{\epsilon}^{pl}\right)$, in these models is used to relate the equivalent plastic strain, $\bar{\epsilon}^{pl}$,  to the plastic strain in rate form: 

\begin{equation}
    \dot{\bar{\epsilon}}^{pl}  = 
    \boldsymbol{h}\left(\bar{\boldsymbol{\sigma}}, \bar{\epsilon}^{pl}\right):
    \dot{\boldsymbol{\epsilon}}^{pl}
\label{eqn:gen7}
\end{equation}

For the damage models, the damage initiation criteria and evolution equations are different for each material model. In general though, the damage initiation criteria for both material models is strain based and the nature of the damage evolution is assumed to be a function the equivalent plastic strain:

\begin{equation}
D=D\left(\bar{\boldsymbol{\sigma}},\bar{\epsilon}^{pl}\right)
\label{eqn:gen8}
\end{equation}

\subsection{Drucker-Prager Plasticity Model with Ductile Damage}

In this constitutive model, a ductile isotropic damage formulation is prescribed using a modified Johnson-Cook damage initiation criterion and a linear stiffness degradation model. In addition to damage, the elasto-plastic behaviour of the rock is also considered using an extended Drucker-Prager model with a linear yield criterion and a Barcelona hardening function. 

The Drucker-Prager plasticity model was developed by \citet{drucker_implications_1950} for modelling frictional materials like granular soils and rock. An important aspect of this plasticity model is the use of a pressure dependent yield criterion to account for the increase in yield stress of geomaterials as the in-situ stresses increase. Specifically, the Drucker-Prager material model is formulated and used for materials with compressive yield strength much greater than the tensile yield strength such as one finds in soils and rocks. However, this material model is intended to simulate the material response under essentially monotonic loading which limits the capacity for modeling cyclic loading.

In addition, the Drucker-Prager model is suitable for using in conjunction with progressive damage and failure models. In this formulation, the Johnson-Cook Damage model is used to model the damage evolution of the rock mass \citep{Johnson_1985}. At a sufficiently large scale, the damage response of NFR can be thought of as behaving in a ductile capacity. 

Here, for the extended Drucker-Prager plasticity model, a linear yield function, $F\left(\bar{\boldsymbol{\sigma}}, \bar{\epsilon}^{pl}\right)$, is assumed to be a function of three stress invariants: the Von-Mises equivalent stress, $p\left(\bar{\boldsymbol{\sigma}}\right)$, the hydrostatic stress, $q\left(\bar{\boldsymbol{\sigma}}\right)$, and the third invariant of deviatoric stress, $r\left(\bar{\boldsymbol{\sigma}}\right)$. In addition, the yield function is written in terms of the compressive yield stress, $\sigma_c^y\left(\bar{\epsilon}^{pl}\right)$, which is defined by the hardening function and two material parameters: the friction angle, $\phi$, and a parameter $K$, defined as the ratio of the yield stress in triaxial tension to the yield stress in triaxial compression:

\begin{multline}
F\left(\bar{\boldsymbol{\sigma}}, \bar{\epsilon}^{pl}\right)=
\frac{1}{2}q\left(\bar{\boldsymbol{\sigma}}\right)\left [ 1+\frac{1}{K}-\left [1-\frac{1}{K} \right ]\left [ \frac{r\left(\bar{\boldsymbol{\sigma}}\right)}{q\left(\bar{\boldsymbol{\sigma}}\right)} \right ]^3 \right ] \\
- p\left(\bar{\boldsymbol{\sigma}}\right)\tan\phi - \left[1-\frac{1}{3}\tan\phi \right]\sigma_c^y\left(\bar{\epsilon}^{pl}\right)
\label{eqn:druc1}
\end{multline}

The flow rule in this formulation is non-associated but the flow potential function, $G\left(\bar{\boldsymbol{\sigma}}\right)$, is written in a very similar form as the yield function with dilation angle, $\psi$, in place of the friction angle. As with the yield function, the flow potential function is written in terms of three stress invariants and two material parameters, dilation angle, and $K$:

\begin{equation}
G\left(\bar{\boldsymbol{\sigma}}\right)=
\frac{1}{2}q\left(\bar{\boldsymbol{\sigma}}\right)\left [ 1+\frac{1}{K}-\left [] 1-\frac{1}{K} \right ]\left [\frac{r\left(\bar{\boldsymbol{\sigma}}\right)}{q\left(\bar{\boldsymbol{\sigma}}\right)} \right ]^3 \right ] 
-p\left(\bar{\boldsymbol{\sigma}}\right)\tan\psi
\label{eqn:druc2}
\end{equation}

In addition to the yield function and the flow rule, the hardening rule is assumed to take the form of the Barcelona model \citep{lubliner_plastic-damage_1989}. The Barcelona model allows for material hardening before softening and approaches a yield stress of 0 as the plastic strain increases.  This form of the hardening function can be written in terms of three material parameters, initial compressive yield strength $\sigma_c^{iy}$, $\alpha$, and $\beta$:

\begin{equation}
\sigma_c=\sigma_c^{iy}\left [ \left [1+\alpha \right ] e^{-\beta\bar{\epsilon}^{pl}}-\alpha e^{-2\beta\bar{\epsilon}^{pl}}  \right ]
\label{eqn:druc3}
\end{equation}

The damage initiation criterion for this material model is based on the Johnson-Cook model of ductile damage initiation \citep{Johnson_1985}. The standard Johnson-Cook model assumes the equivalent plastic strain when damage is initiated, $\bar{\epsilon}_{f}^{pl}\left(\eta\right)$, is a function of triaxiality, $\eta$, and is written in terms of five material parameters:

\begin{equation}
\bar{\epsilon}_{f}^{pl}\left(\eta,\dot{\bar{\epsilon}}^{pl},\hat{T}\right)=\left[D_{1}+D_{2}e^{D_{3}\eta}\right]\left[1+D_{4}\ln\left(\frac{\dot{\bar{\epsilon}}^{pl}}{\dot{\bar{\epsilon}}}\right)\right]\left[1+D_{5}\hat{T}\right]\label{eqn:druc7}
\end{equation}

However, assuming isothermal conditions, neglecting rate effects, and assuming a simplified form of the exponential relationship, the initiation criterion can be reduced to two material parameters, $D_2$ and $D_3$:

\begin{equation}
\bar{\epsilon}_{f}^{pl}\left(\eta\right)=D_{2}e^{D_{3}\eta}
\label{eqn:druc4}
\end{equation}

After the material has experienced yield and material damage has occurred, the stress-strain relationship becomes strongly mesh-dependent because of strain localization due to the energy dissipation decreasing as the mesh is refined. As such, \citet{Hillerborg_1976} proposed a stress-displacement response based on fracture energy after damage initiation assuming that evolving damage is a linear degradation of the material stiffness in compression. Assuming a linear form, the effective plastic displacement when the material is completely damaged, $\bar{u}^{pl}_f$, can be specified, and the damage evolution can then be written in terms of the effective plastic displacement, $\bar{u}^{pl}$:

\begin{equation}
\dot{D}=\frac{\dot{\bar{u}}^{pl}}{\bar{u}_{f}^{pl}}
\label{eqn:druc5}
\end{equation}

\subsection{Damage-Plasticity Model for Quasi-Brittle Materials}

In this constitutive model, a quasi-brittle isotropic damage formulation is prescribed using a linear stiffness degradation model that accounts for cyclic loading. In addition to damage, the elasto-plastic behaviour of the rock is also considered using Lubliner's plasticity model with a linear yield criterion and a Barcelona hardening function \citep{lubliner_plastic-damage_1989}. 

This damage-plasticity model was developed by \citet{lubliner_plastic-damage_1989} as a plasticity based damage model for non-linear analysis of concrete failure. Subsequently, \citet{lee_plastic-damage_1998} further developed the model to facilitate cyclic loading by adding a second damage variable and introducing a new yield function to account for the additional damage variable. 

This model was specifically formulated for modeling quasi-brittle materials under low confining stresses subject to cyclic loading. In addition to the separate damage variables governing the stiffness degradation, the stiffness recovery and material hardening/softening is also treated separately in both compression and tension. Because the formulation does not consider the effects of large hydrostatic stresses, the applicability of this plasticity model to in-situ geomechanics at depth may not be sufficiently accurate. As such, this model is more appropriate for shallow geological models that require cyclic loading paths to be considered.

The yield function for this model is based on the yield function proposed by \citet{lee_plastic-damage_1998} which was developed to allow for differential hardening under tension and compression. The resultant yield function, $F\left(\bar{\boldsymbol{\sigma}},\bar{\epsilon}^{pl}\right)$, can be expressed in terms of two stress invariants: the Von-Mises equivalent stress, $p\left(\bar{\boldsymbol{\sigma}}\right)$, the hydrostatic stress, $q\left(\bar{\boldsymbol{\sigma}}\right)$:

\begin{equation}
    F  \left(\bar{\boldsymbol{\sigma}},\bar{\epsilon}^{pl}\right) =\frac{1}{1-A}
    \left[ q\left(\bar{\boldsymbol{\sigma}}\right)-3A p\left(\bar{\boldsymbol{\sigma}}\right)+B\left(\bar{\epsilon}^{pl}\right)
        \left\langle\hat{\bar{\boldsymbol{\sigma}}}\right\rangle-\gamma\left\langle-\hat{\bar{\boldsymbol{\sigma}}}\right\rangle\right]
    -\bar{\sigma}_{c} \left(\bar{\epsilon_{c}}^{pl}\right)
\label{eqn:const10c}
\end{equation}

Where $A$ and $\gamma$ are dimensionless material constants. Experimental testing has yielded values of $A$ between $0.08$ and $0.12$, as well as a typical $\gamma$ value of approximately $3$ \citep{lubliner_plastic-damage_1989}. The hat notation, $\hat{\bar{\boldsymbol{\sigma}}}$, for an arbitrary stress tensor, $\bar{\boldsymbol{\sigma}}$, represents the algebraically maximum eigenvalue, or, the maximum principle stress. In addition, $\left\langle\right\rangle $ are Macauley brackets and can be defined as:

\begin{equation}
\left\langle x\right\rangle =\frac{1}{2}\left(\left|x\right|+x\right)\label{eqn:const9-3}
\end{equation}

The flow rule in this formulation is non-associated which means that the flow potential function, $G\left(\bar{\boldsymbol{\sigma}}\right)$, follows a different form than the yield function. Like with the yield function, the flow potential function, is written in terms of two stress invariants, $p\left(\bar{\boldsymbol{\sigma}}\right)$ and $q\left(\bar{\boldsymbol{\sigma}}\right)$, and two material parameters, dilation angle, $\psi$, and eccentricity $\varepsilon$:

\begin{equation}
G\left(\bar{\boldsymbol{\sigma}}\right)=\sqrt{\left[\varepsilon\sigma^{iy}\tan\psi\right]^{2}+q\left(\bar{\boldsymbol{\sigma}}\right)^{2}}-p\left(\bar{\boldsymbol{\sigma}}\right)\tan\psi\label{eqn:const11c}
\end{equation}

In this formulation of damage-plasticity, the brittle nature of rock necessitates separate characterization of tensile and compressive damage. With quasi-brittle materials such as rock, it has been found that compressive stiffness can be recovered upon crack closure. Conversely, in these materials, tensile stiffness is not recovered after compressive cracks have developed. This behaviour implies that two separate scalar damage values should exist for the given system to account for both the compressive stiffness degradation and the tensile stiffness degradation. As such, the equivalent plastic strain is also considered separately for tension ($\bar{\epsilon}_{t}^{pl}$) and compression ($\bar{\epsilon}_{c}^{pl}$) and is represented as follows: 

\begin{equation}
\bar{\epsilon}^{pl}=
\left[
\begin{array}{c}
    \bar{\epsilon}_{t}^{pl}\\
    \bar{\epsilon}_{c}^{pl}
\end{array}
\right]
\label{eqn:const9}
\end{equation}

The hardening rule for this model is slightly modified from (\ref{eqn:gen7}) to accommodate two hardening variables (equivalent plastic strains) for tension and compression. The hardening rule can thus be written in matrix form:

\begin{equation}
\mathbf{h}\left(\bar{\boldsymbol{\sigma}},\bar{\epsilon}^{pl}\right)=\left[\begin{array}{ccc}
r\left(\hat{\bar{\sigma}}_{ij}\right)\frac{\sigma_t\left(\bar{\epsilon}_{t}^{pl}\right)}{g_t} & 0 & 0\\
0 & 0 & -\left(r\left(\hat{\bar{\sigma}}_{ij}\right)-1\right)\frac{\sigma_c\left(\bar{\epsilon}_{c}^{pl}\right)}{g_c}
\end{array}\right]\label{eqn:const9-1}
\end{equation}

Where $\sigma_t$ and $\sigma_c$ are the yield stresses in tension and compression as specified by the hardening curves which describe the evolution of the equivalent plastic strains. The compressive hardening rule is approximated here using the Barcelona model in a similar capacity as was done for the Drucker-Prager model in the previous section. The only difference here being that the hardening is defined in terms of the inelastic strain, $\bar{\epsilon}^{in}$, rather than the plastic strain before: 

\begin{equation}
\sigma_c=\sigma_c^{iy}\left [ \left [1+\alpha \right ] e^{-\beta\bar{\epsilon}^{in}}-\alpha e^{-2\beta\bar{\epsilon}^{in}}  \right ]
\label{eqn:dam1b}
\end{equation}

There exists a subtle but important distinction between these two strain measurements when considering CDM. The plastic strain refers to all the strain that is non-elastic (i.e. the remaining strain after the applied stress is unloaded in the damaged state), while the inelastic strain refers to the theoretical plastic strain that would remain if the material was unloaded with the original material stiffness (i.e. in an undamaged state).

The tensile hardening function has a fundamentally different behavior than the compressive hardening function, and is therefore approximated using an exponential function. This function is described by the initial tensile yield stress, $\sigma_{t}^{iy}$, and a decay parameter, $\lambda$. These parameters describe the relationship between the tensile yield stress, $\sigma_{t}$, and the cracking strain, $\bar{\epsilon}^{ck}$ which is the tensile portion of inelastic strain:

\begin{equation}
\sigma_{t}\left(\bar{\epsilon}^{ck}\right)=\sigma_{t}^{iy}e^{\lambda\bar{\epsilon}^{ck}}
\label{eqn:dam1a}
\end{equation}

In addition, $g_t$ and $g_c$ from (\ref{eqn:const9-1}) represent the dissipated fracture energy density during micro-cracking. The use of the dissipated fracture energy density over fracture energy (a material property) stems from the fact that the strain softening part of the stress-strain curve cannot represent a local physical property of the material in addition to being highly mesh sensitive. The dissipated fracture energy densities are defined in terms of a characteristic length, $l$, associated with the mesh size and the fracture energy in tension, $G_t$, and compression, $G_c$: 

\begin{equation}
g_t = \frac{G_t}{l}
\label{eqn:dam1c}
\end{equation}

\begin{equation}
g_c = \frac{G_c}{l}
\label{eqn:dam1d}
\end{equation}

Furthermore, the weighting function, $r\left(\hat{\bar{\boldsymbol{\sigma}}}\right)$, weights the hardening functions depending on the degree of tension or compression that the model is experiencing:

\begin{equation}
r\left(\hat{\bar{\boldsymbol{\sigma}}}\right)=\frac{\sum_{i=1}^{3}\left\langle \hat{\bar{\boldsymbol{\sigma}}}_{i}\right\rangle }{\sum_{i=1}^{3}\left|\hat{\bar{\boldsymbol{\sigma}}}_{i}\right|},\qquad0\leq r\left(\hat{\bar{\boldsymbol{\sigma}}}\right)\leq1
\label{eqn:const9-2}
\end{equation}

Loading a quasi-brittle material in compression or tension causes damage in the material, which reduces the effective stiffness, weakening the unloading response. This damage is characterized by two damage variables, one of which represents the damage due to tensile loading, $D_{t}$, the other represents damage due to compressive loading, $D_{c}$. 

\begin{equation}
D_{t}=D_{t}\left(\bar{\epsilon}_{t}^{pl}\right),\qquad0\leq D_{t}\leq1
\label{eqn:dam2a}
\end{equation}

\begin{equation}
D_{c}=D_{c}\left(\bar{\epsilon}_{c}^{pl}\right),\qquad0\leq D_{c}\leq1
\label{eqn:dam2b}
\end{equation}

The damage in both compression and tension is a necessarily increasing
function of the equivalent plastic strains. For cyclic loading, both the compressive and tensile damage need to be considered. Two stiffness recovery factors are introduced, $s_{t}$ and $s_{c}$, which represent the stiffness recovery effects associated with stress reversals. The damage can be said to take the form of:

\begin{equation}
\left[1-D\right]=\left[1-s_{t}D_{c}\right]\left[1-s_{c}D_{t}\right],\qquad0\leq s_{t},s_{c}\leq1\label{eqn:dam4}
\end{equation}

%----------------------------------------------------------------------
\section{Parameter Estimation Algorithms}
%----------------------------------------------------------------------

Parameter estimation involves a process of obtaining a parameter set $\boldsymbol{\chi}$ of a CDM model that minimizes the difference between $\boldsymbol{\sigma}^M$-$\boldsymbol{\epsilon}^M$ and $\left<\boldsymbol{\sigma}\right>$-$\left<\boldsymbol{\epsilon}\right>$ for all load paths. Herein, the parameter estimation was conducted using calibration algorithms, a subset of optimization which attempts to minimize a least-squares objective function \citep{matott_ostrich:_2008}. Optimization algorithms are often described as either deterministic, which find the local optimum precisely, or heuristic, which find the global optimum approximately.

Deterministic optimization algorithms primarily focus on searching for the optima within the local parameter space by iteratively converging towards a solution. Heuristic optimization algorithms explore the entire parameter space approximately and provide an estimate of the global optima. Heuristic techniques are useful for highly non-linear problems, where there are numerous local optima within the prescribed parameter space. When searching the global parameter-space deterministically becomes too computationally demanding, heuristic methods are used, at the cost of completeness and accuracy. A compromise between speed and accuracy can be obtained by strategically using both types of algorithms.

A combination of two optimization algorithms is used to assess the optimal parameter set. An initial heuristic algorithm is applied to search for the approximate global optima, followed by a deterministic algorithm as a local refinement of the optimal parameter set. Particle Swarm Optimization (PSO) is used for the global heuristic search, whereas the Levenberg-Marquardt Algorithm (LMA) is used for the local deterministic search. 

Consider a $j^{th}$ arbitrary homogenized stress measurement, $\left< s \right>_j$,  at any given simulation time which acts as the target solution for the macroscale model. The same stress measurement in the macroscale model, $s^M_j \left(\boldsymbol{\chi}, \left< e \right>\right)$, can be expressed as a function of the parameter set for the continuum constitutive model, $\boldsymbol{\chi}$, containing $n$ number of parameters, and the homogenized strain at the given load step, $\left< e \right>$. Here, the weighted least-squares objective function to be minimized, $\Psi$, can be written in terms of a weighting parameter, $w_j$, for $m$ number of stress measurements:

\begin{equation}
\Psi=\sum_{j=1}^m\left[w_j\left[\left< s \right>_j-s^M_j \left(\boldsymbol{\chi}, \left< e \right>\right)\right]\right]^2
\label{eqn:lma2}
\end{equation}

The aim of the following optimization algorithms can be described as an attempt to minimize $\Psi$ by varying $\boldsymbol{\chi}$, subject to the constraint that all parameters values within $\boldsymbol{\chi}$ represent physically realistic values and fall within specified bounds.

The parameter estimation works by iteratively running a single element CDM model, subject to boundary conditions provided by the homogenized DEM simulations, with successive parameter sets that intelligently adapt in order to converge to the DEM data. The approach used here is similar in aim to the Least Squares method that was briefly described in \citet{marquardt_algorithm_1963}.

\subsection{Particle Swarm Optimization (PSO)}

The PSO algorithm is a heuristic optimization algorithm that was developed by \citet{Kennedy} as a byproduct of modeling the cooperative-competitive nature of social behaviour in birds as they flocked searching for food. The PSO algorithm, in a conceptual sense, consists of a series of 'particles' (birds) which 'swarm' through the entire parameter space (sky) searching for the global optima (food) using a combination of individual 'particle' knowledge and global 'swarm' (flock) knowledge.

Consider a particle in an $n$ dimensional parameter space with an arbitrary velocity, $\vec{v}_i$, and position, $\vec{x}_i$. At each position in this parameter space, $\Psi$ can be evaluated with an attempt to find the position that minimizes $\Psi$. The motion of this particle allows the the exploration of the parameter space to find a minimum value of $\Psi$ and the corresponding position. After a period of time, $\Delta t$, the new position of the particle, $\vec{x}_{i+1}$, can be written as a combination of the old position and the new velocity, $\vec{v}_{i+1}$. 

\begin{equation}
\vec{x}_{i+1} = \vec{x}_i + \vec{v}_{i+1} \Delta t
\label{eqn:psoupdate}
\end{equation}

Here, $\vec{v}_{i+1}$, is considered to be influenced by $\vec{v}_i$, the position of the current local optimum, $p_l$, and the position of the current global optimum, $p_g$. The local optimum refers to the minimum value of $\Psi$ observed by the individual particle, while the global optimum refers to the minimum value of $\Psi$ observed by all particles. As such,  $\vec{v}_{i+1}$ is written as a linear combination of $\vec{v}_{i}$, the velocity required to move the particle back to the local optimum and the velocity required to move the particle back to global optimum. In order to prevent the algorithm from oscillating indefinitely in a predictable manner, a randomization vector, $\vec{U}\left(\phi\right)$, of length $n$ is introduced to provide coefficients between 0 and $\phi$ to the velocity vectors. The operator $\odot$ refers to the Hadamard product (element-wise multiplication):

\begin{equation}
\vec{v}_{i+1} = \vec{v}_i + \frac{\vec{U}_i\left(\phi_1\right)\odot\left[\vec{p}_l-\vec{x}_i\right] + \vec{U}_i\left(\phi_2\right)\odot\left[\vec{p}_g-\vec{x}_i\right]}{\Delta t}
\label{eqn:psobasic}
\end{equation}

As one would expect, the behaviour of the PSO is highly sensitive to the chosen values of $\phi_1$ and $\phi_2$. If these parameters are too small, then the optimization becomes "unresponsive" such that the initial velocity is maintained and successive iterations do not have the capacity to appreicably change their velocity to search the parameter space effectively. Alternatively, if these parameters are too large, the PSO has the capacity to become unstable such that the particle speeds keep increasing on successive iterations. Commonly accepted in most PSO algorithms is the assumption that $\phi_1 = \phi_2 = 2$. To overcome these limitations and control the scope of the search, \citet{Shi} introduced the inertial weight term, $\omega$:

\begin{equation}
\vec{v}_{i+1} = \omega\vec{v}_i + \frac{\vec{U}_i\left(\phi_1\right)\odot\left[\vec{p}_l-\vec{x}_i\right] + \vec{U}_i\left(\phi_2\right)\odot\left[\vec{p}_g-\vec{x}_i\right]}{\Delta t}
\label{eqn:psoinertia}
\end{equation}

This inertial weight acts as a scalar multiplier between $0$ and $1$ for $\vec{v}_i$, and can be interpreted as a measure of the fluidity of the system. A large inertial term allows the particle to maintain it's current velocity to a higher degree indicating a system with low viscosity lending to a more explorative search, while a small inertial term dissipates the particles velocity more rapidly indicating a more viscous system which favours exploitative searching. 

Additional damping can be provided in the form of a constriction coefficient which controls the convergence of the particle, by ensuring convergence and preventing explosion. \citet{Clerc_2002} noted that many means of constricting the velocity function exist, but provided a simple form of the constriction, using a constriction coefficient, $\zeta\left(\phi_1, \phi_2\right)$. 

\begin{equation}
\vec{v}_{i+1} = \zeta\left(\phi_1, \phi_2\right) \left[\omega\vec{v}_i + \frac{\vec{U}_i\left(\phi_1\right)\odot\left[\vec{p}_l-\vec{x}_i\right] + \vec{U}_i\left(\phi_2\right)\odot\left[\vec{p}_g-\vec{x}_i\right]}{\Delta t}\right]
\label{eqn:psoconstriction}
\end{equation}

Here, $\zeta\left(\phi_1, \phi_2\right)$ is given by the following, where $\phi=\phi_1+\phi_2$:

\begin{equation}
\zeta\left(\phi_1, \phi_2\right) = \frac{2}{\phi-2+\sqrt{\phi^2-4\phi}}
\label{eqn:psoconstriction2}
\end{equation}

In addition, with the desire to give more credence to either the local optimum or the global optimum, two "trust" parameters are introduced. $c_1$ is referred to as the cognitive parameter as it weights the particles own experience, while the second parameter, $c_2$, is referred to as the social parameter as it weights the influence of the combined experience of the swarm:

\begin{equation}
\vec{v}_{i+1} = \zeta\left(\phi_1, \phi_2\right) \left[\omega\vec{v}_i + \frac{c_1\vec{U}_i\left(\phi_1\right)\odot\left[\vec{p}_l-\vec{x}_i\right] + c_2\vec{U}_i\left(\phi_2\right)\odot\left[\vec{p}_g-\vec{x}_i\right]}{\Delta t}\right]
\label{eqn:psoposition}
\end{equation}

In terms of the up-scaling application, the position of the particle can be taken to represent an estimate of the optimal parameter set, $\boldsymbol{\chi}_i$, while the velocity of the particle represents the direction and magnitude of the change in the parameter set estimate for the next iteration $\Delta\boldsymbol{\chi}_{i+1}$. Furthermore, the time step can considered to be a unit iteratation step, which allows for (\ref{eqn:psoposition}) to be abstracted as follows in the context of up-scaling DEM simulations.

\begin{equation}
\Delta\boldsymbol{\chi}_{i+1} = \zeta\left(\phi_1, \phi_2\right) \left[\omega\Delta\boldsymbol{\chi}_{i} + c_1\vec{U}_i\left(\phi_1\right)\odot\left[\boldsymbol{\chi}_l-\boldsymbol{\chi}_i\right] + c_2\vec{U}_i\left(\phi_2\right)\odot\left[\boldsymbol{\chi}_g-\boldsymbol{\chi}_i\right]\right]
\label{eqn:psoposition2}
\end{equation}

Where $\boldsymbol{\chi}_l$ is the parameter set corresponding to the minimum value of $\Psi$ for the particle, and $\boldsymbol{\chi}_g$ is the parameter set corresponding to the minimum value of $\Psi$ for all particles. In addition, (\ref{eqn:psoupdate}) can be rewritten in a similar capacity:

\begin{equation}
\boldsymbol{\chi}_{i+1}  = \boldsymbol{\chi}_{i}  +  \Delta\boldsymbol{\chi}_{i+1}
\label{eqn:psoupdate2}
\end{equation}

For a particle swarm containing an arbitrary number of particle, the optimal parameter set of the system, $\boldsymbol{\chi}$, is  considered to be $\boldsymbol{\chi}_g$ after a specified number of iterations, or once all the particles converge to a stable solution. The general PSO algorithm as described here is summarized as follows:

\begin{enumerate}
\item Assign particles random positions and velocities in the parameter space
\item Move each particle with (\ref{eqn:psoposition2}) and (\ref{eqn:psoupdate2})
\item For each particle, revise $\boldsymbol{\chi}_l$ if new local optimum found
\item Revise $\boldsymbol{\chi}_g$ if new global optimum found
\item If current iteration is greater than the maximum number of iterations or solution is stable, iteration is complete. Otherwise, go to step 2
\end{enumerate}

\subsection{Asynchronous Parallel PSO (APPSO)}

The PSO described above possesses a large number of attractive qualities for parameter estimation which make it desirable to for up-scaling.  However, the main drawback of the PSO is the large computational costs in terms of total elapsed time primarily due to the fact that the algorithm was originally designed for a serial implementation. The serial implementation, although effective in it's own right, can be dramatically improved through parallelization. 

The nature of the PSO lends itself to a fairly trivial implementation of a synchronous parallellization scheme which does not require changing the nature of the algorithm. Here, since all the particles at each iteration are treated independently, the updated positions and corresponding objective functions can be computed in parallel. This parallelization scheme waits for all the particles to complete their analysis before moving on to the next iteration. As a result, the parallel efficiency is often compromised due to processors having to wait for the final particle(s) to finish their analysis. This idleness of the processors can be caused by having a swarm size that is not an integer multiple of the number of processors, having a heterogenous computing environment where processors have different computational speeds, or having a numerical simulation that requires different amount of computational time depending on the input parameters \citep{Venter_2006}. This inefficiency of this synchronous parallelization increases as the number of processors increases due to the increasing number of idle processors towards the end of the iteration.

To overcome these parallel inefficiencies, \citet{Venter_2006} introduced an Asynchronous Parallel Particle Swarm Optimization (APPSO) algorithm to continuously use the available processors with the goal of having no idle processors from one iteration to the next. The key here, is to separate the update calculations associated with each point and those associated with the swarm as a whole. Normally, in the synchronous scheme, the update calculations (i.e. updating $\boldsymbol{\chi}_l$ and $\boldsymbol{\chi}_g$) are done at the end of each iteration. In this asynchronous scheme, updating $\boldsymbol{\chi}_l$ remains the same, but updating $\boldsymbol{\chi}_g$ uses the best position from the previous iteration instead of the current iteration in order perform the update calculations immediately and allow the analyses to proceed without waiting for the rest of the particles to complete their analysis.

\subsection{Levenburg-Marquardt Algorithm (LMA)}

The Levenberg-Marquardt Algorithm (LMA) is a deterministic optimization algorithm that was proposed by \citet{marquardt_algorithm_1963} which builds off of the work of \citet{levenberg_method_1944}. This calibration algorithm combines a quasi-Newton approach with a conjugate gradient technique in order to efficiently minimize non-linear least-squares problems. 

For notational simplicity (\ref{eqn:lma2}) is rewritten in matrix form by considering a vector containing $m$ number of arbitrary homogenized stress measurements at any given time, $\left< \mathbf{s} \right>$, and a corresponding vector of stress measurements for the macroscale model, $\mathbf{s}^M$:

\begin{equation}
\left< \mathbf{s} \right>=\left[\left< s \right>_1, \left< s \right>_2, \dots, \left< s \right>_m \right]^T
\label{eqn:lma3}
\end{equation}

\begin{equation}
\mathbf{s}^M=\left[s^M_1, s^M_2, \dots, s^M_m \right]^T
\label{eqn:lma4}
\end{equation}

This vectorization facilitates writing the weighted least-squares objective function in matrix form using $\mathbf{Q}$ as a weighting matrix:

\begin{equation}
\Psi=\left[\left<\mathbf{s}\right>-\mathbf{s^M}\right]^T \mathbf{Q} \left[\left<\mathbf{s}\right>-\mathbf{s^M}\right]
\label{eqn:lma5}
\end{equation}

Where $\mathbf{Q}$ is written as a diagonal matrix containing the weighting parameters:

\begin{equation}
\mathbf{Q}=\begin{bmatrix}
w_1^2 & 0     & \dots  & 0\\ 
0     & w_2^2 &        & 0\\ 
\vdots&       & \ddots & \vdots \\ 
0     & 0     & \dots  & w_m^2
\end{bmatrix}
\label{eqn:lma6}
\end{equation}

The first step in the LMA formulation considers an arbitrary initial set of parameters, $\boldsymbol{\chi}_0$, and the corresponding macroscale stress measurements, $\mathbf{s}^M_0$. The relationship between $\boldsymbol{\chi}$ and $\mathbf{s}^M$ is generally highly non-linear, so the function is approximated with a taylor series expansion about $\boldsymbol{\chi}_0$, yielding the following linearization:

\begin{equation}
\mathbf{s}^M \approx \mathbf{s}^M_0 + \mathbf{J} \left[ \boldsymbol{\chi}-\boldsymbol{\chi}_0 \right]
\label{eqn:lma7}
\end{equation}

Where the partial derivatives of the given set of $s^M$ stress measurements with respect to the parameters in $\boldsymbol{\chi}$ are represented in the Jacobian matrix, $\mathbf{J}$: 

\begin{equation}
\mathbf{J}=\begin{bmatrix}
\frac{\partial s^M_1}{\partial \boldsymbol{\chi}_1} & \frac{\partial s^M_1}{\partial \boldsymbol{\chi}_2}     & \dots  & \frac{\partial s^M_1}{\partial \boldsymbol{\chi}_n}\\ 
\frac{\partial s^M_2}{\partial \boldsymbol{\chi}_1}     & \frac{\partial s^M_2}{\partial \boldsymbol{\chi}_2} &        & \frac{\partial s^M_2}{\partial \boldsymbol{\chi}_n}\\ 
\vdots&       & \ddots & \vdots \\ 
\frac{\partial s^M_m}{\partial \boldsymbol{\chi}_1}     & \frac{\partial s^M_m}{\partial \boldsymbol{\chi}_2}     & \dots  & \frac{\partial s^M_m}{\partial \boldsymbol{\chi}_n}
\end{bmatrix}
\label{eqn:lma8}
\end{equation}

Substituting (\ref{eqn:lma7}) into (\ref{eqn:lma5}) gives an linearized approximation of the objective function:

\begin{equation}
\Psi\approx\left[\left<\mathbf{s}\right>-\left[\mathbf{s}^M_0 + \mathbf{J} \left[ \boldsymbol{\chi}-\boldsymbol{\chi}_i \right] \right]\right]^T \mathbf{Q} \left[\left<\mathbf{s}\right>-\left[\mathbf{s}^M_0 + \mathbf{J} \left[ \boldsymbol{\chi}-\boldsymbol{\chi}_0 \right]\right]\right]
\label{eqn:lma9}
\end{equation}

Since (\ref{eqn:lma9}) is still an approximation of the objective function, an iterative approach is required to converge to an optimal estimate of $\boldsymbol{\chi}$. Here, (\ref{eqn:lma9}) can be modified and written in an iterative capacity such that the current parameter set estimate, $\boldsymbol{\chi}_i$, simulated stress measurements, $\mathbf{s}^M_i$, objective function value, $\Psi_i$, and Jacobian matrix, $\mathbf{J}_i$ can be used to estimate the next parameter set estimate $\boldsymbol{\chi}_{i+1}$:

\begin{equation}
\Psi_{i}=\left[\left<\mathbf{s}\right>-\left[\mathbf{s}^M_i + \mathbf{J}_i \left[ \boldsymbol{\chi}_{i+1}-\boldsymbol{\chi}_i \right] \right]\right]^T \mathbf{Q} \left[\left<\mathbf{s}\right>-\left[\mathbf{s}^M_i + \mathbf{J}_i \left[ \boldsymbol{\chi}_{i+1}-\boldsymbol{\chi}_i \right]\right]\right]
\label{eqn:lma9a}
\end{equation}

The vector $\left[ \boldsymbol{\chi}_{i+1}-\boldsymbol{\chi}_0 \right]$, which represents the difference in the current estimate of the parameter set and the next parameter set estimate is termed the upgrade vector. By taking the derivative of $\Psi$ with respect to $\boldsymbol{\chi}$, the upgrade vector can be written as:

\begin{equation}
\left[ \boldsymbol{\chi}_{i+1}-\boldsymbol{\chi}_i \right] = \left[\mathbf{J}_i^T\mathbf{Q}\mathbf{J}_i\right]^{-1}\mathbf{J}_i^T\mathbf{Q}\left[\left<\mathbf{s}\right>-\mathbf{s}^M_i\right]
\label{eqn:lma10}
\end{equation}

Here, for convenience, the upgrade vector is written as $\mathbf{U}_{i} = \left[ \boldsymbol{\chi}_{i+1}-\boldsymbol{\chi}_i \right]$. Since (\ref{eqn:lma10}) is still an approximation of the upgrade vector, an iterative approach to finding $\boldsymbol{\chi}$ is required. The LMA further proposes that $\boldsymbol{U}_i$ be modified with the Marquardt parameter, $\alpha$:

\begin{equation}
\mathbf{U}_{i} = \left[\mathbf{J}_i^T\mathbf{Q}\mathbf{J}_i+\alpha\mathbf{I}\right]^{-1}\mathbf{J}_i^T\mathbf{Q}\left[\left<\mathbf{s}\right>-\mathbf{s}^M_i\right]
\label{eqn:lma11}
\end{equation}

Here, $\mathbf{I}$ is the $n\times n$ identity matrix. The Marquardt parameter in (\ref{eqn:lma11}) allows $\mathbf{U}_{i}$ to approximate a Steepest-Descent Method (SDM) for large values of $\alpha$, while using a Taylor Series Approximation (TSA) for small values of alpha. This formulation allows for a smooth transition between a SDM when the parameter set estimate is far away from the optimal parameter set, and a TSA when the parameter set estimate is close to the optimal parameter set.

Another key development in the LMA notes that for many calibration and parameter estimation problems, elements within $\mathbf{s}^M_i$ and $\left<\mathbf{s}\right>$ may differ by several orders of magnitude. Such large variations can lead to significant roundoff error during the calculation of $\mathbf{J}_i$. This error can be avoided by introducing a scaling matrix, $\mathbf{S}_i$:

\begin{equation}
\mathbf{S}_i=\begin{bmatrix}
\frac{1}{J_{i_{11}}w_1} & 0    & \dots  & 0\\ 
0 & \frac{1}{J_{i_{22}}w_2} &        & 0\\ 
\vdots&       & \ddots & \vdots \\ 
0    & 0    & \dots  & \frac{1}{J_{i_{nn}}w_n}
\end{bmatrix}
\label{eqn:lma12}
\end{equation}

With (\ref{eqn:lma12}), the upgrade vector in (\ref{eqn:lma11}) can be rewritten in a mathematically identical way while avoiding numerical errors:

\begin{equation}
\mathbf{U}_i= \mathbf{S}_i\left[\mathbf{S}_i^T\mathbf{J}_i^T\mathbf{Q}\mathbf{J}_i\mathbf{S}_i+\alpha\mathbf{S}_i^T\mathbf{S}_i\right]^{-1}\mathbf{S}_i^T\mathbf{J}_i^T\mathbf{Q}\left[\left<\mathbf{s}\right>-\mathbf{s}^M_i\right]
\label{eqn:lma13}
\end{equation}

The final feature of the LMA is the introduction of the Marquardt Lambda, $\lambda$. The Marquardt Lambda is taken as the largest term in the matrix $\alpha\mathbf{S}^T\mathbf{S}$. Here, the adjustment of $\lambda$ provides control over the relative weighting of the SDM vs the TSA such that for large values of $\lambda$, the SDM dominates, while for small values of $\lambda$, the TSA dominates. The iterative algorithm is summarized as follows \citep{matott_ostrich:_2008}:

\begin{enumerate}
\item Choose initial value for $\lambda$
\item Compute $\mathbf{s}^M \left(\boldsymbol{\chi}_i, \left< e \right>\right)$
\item Compute $\Psi_i$ with (\ref{eqn:lma9a})
\item Compute $\mathbf{U}_i$ with (\ref{eqn:lma13})
\item Compute $\boldsymbol{\chi}_{i+1}=\boldsymbol{\chi}_i+\mathbf{U}_i$
\item Compute $\mathbf{s}^M \left(\boldsymbol{\chi}_{i+1}, \left< e \right>\right)$
\item Compute $\Psi_{i+1}$ with (\ref{eqn:lma9a})
\item Adjust $\lambda$
\begin{enumerate}
\item If number of $\lambda$ adjustments is optimal or exceeds maximum, go to step 9
\item If $\Psi_{i+1} < \Psi_i$, reduce $\lambda$, increment $i$, and go to step 2
\item If $\Psi_{i+1} \geq \Psi_i$, increase $\lambda$, and go to step 4
\end{enumerate}
\item Test for convergence by comparing $\Psi_{i+1}$ and $\Psi_i$
\begin{enumerate}
\item If converged, iteration is complete and $\boldsymbol{\chi}_{i+1}$ represents the optimal solution.
\item If not converged, increment $i$, and go to step 2
\end{enumerate}
\end{enumerate}

%----------------------------------------------------------------------
\section{Physically Meaningful Model Parameterization}
%----------------------------------------------------------------------

To accelerate the process of finding a near-optimal set of parameters, it is important to limit the search space of the parameterization algorithm. This is especially important when the number of parameters is large. We have found that it is beneficial for the parameters to have physical meaning in order to specify realistic bounds.

Here, the parameterization of the two macorscale contiuum models is presented. For both models, the approach to paramaeterization is the same, but the specific methods and resulting parameters are different due to the inherently different constitutive models. However, the elastic behaviour for both models is governed by the same constitutive relation. The elastic behaviour is parameterized by Young's modulus, $E$, and Poisson's ratio, $\nu$. Bounds on these quantities are well known. 

\subsection{Drucker-Prager Model with Ductile Damage}

The yield function and flow potential function are parameterized in terms of the friction angle, dilation angle, and the stress ratio $K$. Bounds on these quantities are relatively well known.

The hardening function (\ref{eqn:druc3}) is given in terms of two empirical coefficients $\alpha$ and $\beta$ and the initial compressive yield stress $\sigma_c^{iy}$.  While it is possible to set bounds on $\sigma_c^{iy}$, it is less straightforward to set bounds for $\alpha$ and $\beta$ as they do not have obvious physical meaning. The hardening function for the Barcelona model is shown in Figure \ref{fig:barcelona}. The coefficients $\alpha$ and $\beta$ can be rewritten in terms of the peak compressive yield strength, $\sigma_{c}^{p}$, the plastic strain at the peak compressive yield strength, $\epsilon_c^{p}$, and the initial compressive yield stress $\sigma_c^{iy}$:

\begin{figure}[!htb]
\begin{center}
\includegraphics[width=0.8\textwidth]{figures/Chapter3/BarcelonaCurve}
\caption{{\label{fig:barcelona}Compressive hardening/sofening function from the Barcelona model. The curve is able to be parameterized using three parameters.%
}}
\end{center}
\end{figure}

\begin{equation}
\beta=\frac{\ln\left[\frac{2\alpha}{1+\alpha} \right ]}{\epsilon_c^{p}}
\label{eqn:param2-2}
\end{equation}

\begin{equation}
\alpha =\frac{2\sigma_c^{p}-\sigma_c^{iy}+2\sqrt{-\sigma_c^p\left[\sigma_c^{iy}-\sigma_c^p \right ]}}{\sigma_c^{iy}}
\label{eqn:param2-3}
\end{equation}

Thus, we parameterize the hardening law in terms of $\sigma_{c}^{p}$, $\epsilon_c^{p}$, and  $\sigma_c^{iy}$ so that bounds can be more easily defined.

Similarly, the Johnson-Cook damage initiation criterion from (\ref{eqn:druc4}) is described by two empirical coefficients ($D_2$, and $D_3$) which do not have intuitive physical meaning. To make setting the bounding limits during the parameter estimation simpler, the Johnson-Cook parameters were rewritten in terms of equivalent plastic strain at which damage is initiated at triaxialities of -0.5 and -0.6, $\bar{\epsilon}^{pl}_{y_{-0.5}}$ and $\bar{\epsilon}^{pl}_{y_{-0.6}}$, respectively:

\begin{equation}
D_2=\frac{\left[\bar{\epsilon}^{pl}_{y_{-0.5}}\right]^6}{\left[\bar{\epsilon}^{pl}_{y_{-0.6}}\right]^5}
\label{eqn:dparam8}
\end{equation}

\begin{equation}
D_3=10\ln \left [\frac{\bar{\epsilon}^{pl}_{y_{-0.5}}}{\bar{\epsilon}^{pl}_{y_{-0.6}}}\right ]
\label{eqn:dparam9}
\end{equation}

In addition, the damage evolution was parameterized using only the plastic displacement at failure parameter. The goal of the parameter estimation module, in the verification examples, is therefore to estimate the 11 parameters $\boldsymbol{\chi}=\{E,\nu,\psi, K, \phi, \sigma_c^{iy},\allowbreak\sigma_{c}^{p}, \epsilon_c^{p}, \bar{\epsilon}^{pl}_{y_{-0.5}},\allowbreak \bar{\epsilon}^{pl}_{y_{-0.6}},\bar{u}^{pl}_f \}$, which are summarized in Table \ref{tab:druckerParameters}.

\begin{table}[!htb]
\centering
\caption{{Parameter set for Drucker-Prager Material Model with Ductile Damage}}
\label{tab:druckerParameters}
\begin{tabularx}{\textwidth}{@{}YccY@{}}
\toprule
\multicolumn{2}{c}{\textbf{Parameter Type}}                                                                      & \textbf{Name}                              & \textbf{Symbol}                   \\ \midrule
\multicolumn{2}{c}{\multirow{2}{*}{Elastic}}                                                                     & Young's Modulus                            & $E$                               \\
\multicolumn{2}{c}{}                                                                                             & Poisson's Ratio                            & $\nu$                             \\ \cmidrule{1-2}
\multirow{6}{*}{Plastic} & \multirow{3}{*}{Flow Rule/Yield Function} 											 & Dilation Angle                             & $\psi$                            \\
                         &                                                                                       & Yield Stress Ratio                         & $K$                               \\
                         &                                                                                       & Friction Angle                             & $\phi$                           \\ \cmidrule{2-2}
                         & \multirow{3}{*}{Hardening Rule}                                                       & Initial Compressive Yield Strength         & $\sigma_c^{iy}$                   \\
                         &                                                                                       & Peak Compressive Yield Strength & $\sigma_c^{p}$                    \\
                         &                                                                                       & Strain at Peak Compressive Yield           & $\epsilon_c^{p}$                 \\ \cmidrule{1-2}
\multirow{3}{*}{Damage}  & \multirow{2}{*}{Initiation}                                                           & Yield Strain at $-0.5$ Triaxiality         & $\bar{\epsilon}^{pl}_{y_{-0.5}}$  \\
                         &                                                                                       & Yield Strain at $-0.6$ Triaxiality        & $\bar{\epsilon}^{pl}_{y_{-0.6}}$ \\ \cmidrule{2-2}
                         & Evolution                                                                             & Plastic Displacement at Failure            & $\bar{u}^{pl}_f$                  \\ \bottomrule
\end{tabularx}
\end{table}

\subsection{Damage-Plasticity Model for Quasi-Brittle Materials}

The yield function for this model is written in terms of three material parameters: $A$, $B$, and $\gamma$. These material parameters are not directly measurable, but can be expressed in terms of measurable parameters. $A$ is expressed here as a function of the ratio $f_0$, which is defined as the ratio of the initial biaxial compressive yeild strength, $\sigma_{b0}$ to the initial uniaxial compressive strength, $\sigma_{c0}$:

\begin{equation}
f_0 = \frac{\sigma_{b0}}{\sigma_{c0}}
\label{eqn:param1a}
\end{equation}

\begin{equation}
A = \frac{f_0-1}{2f_0-1}
\label{eqn:param1}
\end{equation}

Additionally, $\gamma$ is expressed in terms of the ratio $K_c$ which expresses the ratio of hydrostatic pressure at yield in tension to the hydrostatic pressure at yield in compression:

\begin{equation}
\gamma = \frac{3\left[1-K_c \right]}{2K_c-1}
\label{eqn:param1-3}
\end{equation}

Similarily, $B$ is expressed as a function of $A$ and the two yield stresses from the hardening rule in (\ref{eqn:const9-1}), $\bar{\sigma}_c\left(\bar{\epsilon}^{pl}_c\right)$ and $\bar{\sigma}_c\left(\bar{\epsilon}^{pl}_t\right)$.

\begin{equation}
B = \frac{\bar{\sigma}_c\left(\bar{\epsilon}^{pl}_c\right)}{\bar{\sigma}_t\left(\bar{\epsilon}^{pl}_t\right)}\left(1-A \right )-\left( 1+A \right)
\label{eqn:param1-2}
\end{equation}

The compressive hardening function, similar to the hardening function in the Drucker-Prager model in the previous section, is also approximated using the Barcelona model as shown in Figure \ref{fig:barcelona}. The same parameterization scheme is also used to write $\alpha$ and $\beta$ in terms of the peak compressive yield strength, $\sigma_{c}^{p}$, and the plastic strain at the peak compressive yield strength, $\epsilon_c^{pp}$:

\begin{equation}
\beta=\frac{\ln\left[\frac{2\alpha}{1+\alpha} \right ]}{\epsilon_c^{in}}
\label{eqn:c1}
\end{equation}

\begin{equation}
\alpha =\frac{2\sigma_c^{p}-\sigma_c^{iy}+2\sqrt{-\sigma_c^p\left[\sigma_c^{iy}-\sigma_c^p \right ]}}{\sigma_c^{iy}}
\label{eqn:c2}
\end{equation}

The tensile hardening rule has a fundamentally different behavior than the compressive hardening rule, and was therefore approximated using an exponential function (Fig \ref{fig:tensionHardening}). The exponential function required only two parameters to characterize the curve completely. The first parameter was the initial tensile yield stress, $\sigma_{t}^{iy}$, which defines the y-intercept of the curve, while the second parameter was the tensile yield stress decay parameter,$\lambda$.

\begin{figure}[!htb]
\begin{center}
\includegraphics[width=0.7\textwidth]{figures/Chapter3/TensionHardening}
\caption{{\label{fig:tensionHardening} Tensile hardening/sofening function. The curve is able to be parameterized using two parameters.%
}}
\end{center}
\end{figure}

In addition to the hardening rules, the damage evolution equations are also parameterized. The compressive damage, $D_c$, is assumed to be a linear function of the inelastic strain through a compressive damage rate parameter, $m$:

\begin{equation}
D_{c}\left(\bar{\epsilon}^{in}\right)=\bar{\epsilon}^{in}m\label{eqn:param3}
\end{equation}

The tensile damage ($D_{t}$) evolution is slightly less trivial, but can also be characterized by a single parameter due to some constraints imposed on the function by the nature of the damage parameter. In
tension, the damage evolution curve starts at the origin and asymptotically approaches $D_{t}=1$ as $\bar{\epsilon}^{ck}\rightarrow\infty$. As such, under this functional assumption, the only parameter required to describe this relationship is the tensile damage rate parameter, n:

\begin{equation}
D_{t}\left(\bar{\epsilon}^{ck}\right)=1-\frac{1}{\left[1+\bar{\epsilon}^{ck}\right]^{n}}\label{eqn:param4}
\end{equation}

Sample damage evolution curves for both tension and compression are illustrated in Fig \ref{fig:damageCurves}, where one can see that the rate at which the tensile damage evolves is far larger than the rate at which the compressive damage evolves. The combination of the elastic parameters, the hardening rule parameters, and the damage evolution parameters, yield a total of 11 parameters that must be identified by experiments or through up-scaling to define the behavior of CDM model.

\begin{figure}[!htb]
\begin{center}
\includegraphics[width=0.7\textwidth]{figures/Chapter3/DamageCurves}
\caption{{\label{fig:damageCurves} Tensile and compressive damage evolution curves.%
}}
\end{center}
\end{figure}

At this point, there are some parameter constraints for the damage evolution that need to be considered for numerical stability. In this model, The damage curves are specified in terms of inelastic strain and cracking strain which need to be converted into plastic strain for the analysis. The inelastic and cracking strains represent the same strain component but refer to compression and tension respectively. This inelastic/cracking strain can be considered as the theoretical plastic strain given that the material is in an undamaged state. The conversion from inelastic/cracking strain to plastic strain is a function of the damaged state at every increment and can be expressed as:

\begin{equation}
\label{eqn:param11}
\bar{\epsilon}_{c}^{pl}=\bar{\epsilon}^{in}-\frac{D_{c}}{1-D_{c}}\frac{\sigma_{c}^{iy}}{E}
\end{equation}

\begin{equation}
\label{eqn:param11-1}
\bar{\epsilon}_{t}^{pl}=\bar{\epsilon}^{ck}-\frac{D_{t}}{1-D_{t}}\frac{\sigma_{t}^{iy}}{E}
\end{equation}

The numerical issues with this formulation arise due to the fact that it is very possible for the converted plastic strain to not be monotonically increasing with respect to the tensile damage. By having a damage evolution curve with a sufficiently steep slope, such that the second term in (\ref{eqn:param11}) increases faster than the first term, it becomes mathematically possible to have decreasing and/or negative plastic strains in the damage evolution definition. As such, the following conditions are applied to constrain the damage evolution to always yield monotonically increasing plastic strains as damage increases:

\begin{equation}
\label{eqn:param6-1}
\frac{d\bar{\epsilon}_{c}^{pl}}{dD_{c}}>0
\end{equation}

\begin{equation}
\label{eqn:param6-2}
\frac{d\bar{\epsilon}_{t}^{pl}}{dD_{t}}>0
\end{equation}

For the compressive damage evolution, we substitute (\ref{eqn:param3}) into (\ref{eqn:param11}) to get the following expression of plastic strain as a function of the compressive damage rate parameter:

\begin{equation}
\bar{\epsilon}_{c}^{pl}=\bar{\epsilon}^{in}-\frac{D_{c}}{1-D_{c}}\frac{\sigma_{c}^{iy}}{E}\\
\label{eqn:param12}
\end{equation}

Combining (\ref{eqn:param12}) and (\ref{eqn:param6-1}) yields the following expression governing the stability limit for the compressive damage rate parameter:

\begin{equation}
m<\frac{\sigma_{c}^{iy}+2E\bar{\epsilon}^{in}-\sqrt{\sigma_{c}^{iy}\left[\sigma_{c}^{iy}+4E\bar{\epsilon}^{in}\right]}}{2E\left[\bar{\epsilon}^{in}\right]^{2}}\label{eqn:param13}
\end{equation}

However, since this upper bound for $m$ is functional on several of the parameterization parameters, which are not constant, the upper bound varies depending on the other input parameters. Because of this, a compressive damage scaling factor, $d_c$, is introduced. In addition, the upper bound for $m$ is dependent on the inelastic strain which is not constant throughout the model. Since only one value of $m$ can be specified for a given simulation, the chosen value of $m$ should be the smallest value over the range of the expected inelastic strain experienced. As can be seen from (\ref{eqn:param13}), as $\bar{\epsilon}^{in} \rightarrow \infty$, $m\rightarrow0$ such that for very large inelastic strains the conversion to plastic strain becomes very unstable. Thus, the compressive damage rate parameter can be written as:

\begin{equation}
m=d_{c}\min_{\bar{\epsilon}^{in}}\left\{\frac{\sigma_{c}^{iy}+2E\bar{\epsilon}^{in}-\sqrt{\sigma_{c}^{iy}\left[\sigma_{c}^{iy}+4E\bar{\epsilon}^{in}\right]}}{2E\left[\bar{\epsilon}^{in}\right]^{2}}\right\}
\label{eqn:param14}
\end{equation}

Where the compressive damage scaling factor has the following limits:

\begin{equation}
0<d_{c}<1\label{eqn:param15}
\end{equation}

The tensile damage evolution curve has the same numerical constraints when converting form cracking strain to plastic strain. Substituting (\ref{eqn:param4}) into (\ref{eqn:param11}) yields the following expression for the plastic strain:

\begin{equation}
\bar{\epsilon}_{t}^{pl}=\bar{\epsilon}^{ck}-\left[\left[1+\bar{\epsilon}^{ck}\right]^{n}-1\right]\frac{\sigma_{t}^{iy}}{E}\label{eqn:param5-1}
\end{equation}

Solving for $n$ with (\ref{eqn:param6-1}) and (\ref{eqn:param5-1}) yields the following inequality governing the upper bound of the tensile damage rate parameter:

\begin{equation}
n<\frac{W\left(\frac{E\left[\bar{\epsilon}^{ck}+1\right]}{\sigma_{t}^{iy}}\ln\left[\bar{\epsilon}^{ck}+1\right)\right]}{\ln\left[\bar{\epsilon}^{ck}+1\right]}\label{eqn:param7}
\end{equation}

Where $W\left(x\right)$ is the Lambert W function defined implicitly as \citep{Corless_1996}:

\begin{equation}
x=W\left(x\right)e^{W(x)}\label{eqn:param8}
\end{equation}

Using the same methodology as was used to derive (\ref{eqn:param14}) in compression, the tensile damage scaling factor can be written as:

\begin{equation}
n=d_{t}\min_{\bar{\epsilon}^{ck}}\left\{\frac{W\left(\frac{E\left[\bar{\epsilon}^{ck}+1\right]}{\sigma_{t}^{iy}}\ln\left[\bar{\epsilon}^{ck}+1\right]\right)}{\ln\left[\bar{\epsilon}^{ck}+1\right]}\right\}
\label{eqn:param9}
\end{equation}

Where the tensile damage scaling factor has the following limits:

\begin{equation}
0<d_{t}<1\label{eqn:param10}
\end{equation}

The goal of the parameter estimation module, is therefore to estimate the 11 parameters $\boldsymbol{\chi}=\{E,\nu,\psi, K_c, \phi, \varepsilon, \sigma_c^{iy},\allowbreak\sigma_{c}^{p}, \epsilon_c^{p}, d_c, d_t \}$, which are summarized in Table \ref{tab:concParam}.

\begin{table}[!htb]
\centering
\caption{Parameters for Damage-Plasticity Model for Quasi-Brittle Materials}
\label{tab:concParam}
\begin{tabularx}{\textwidth}{@{}YccY@{}}
\toprule
\multicolumn{2}{c}{Parameter Type}                         & Name                               & Symbol              \\ \midrule
\multicolumn{2}{c}{\multirow{2}{*}{Elastic}}               & Young's Modulus                    & $E$                 \\
\multicolumn{2}{c}{}                                       & Poisson's Ratio                    & $\nu$               \\ \cmidrule(r){1-2}
\multirow{7}{*}{Plastic} & \multirow{2}{*}{Flow Rule}      & Dilation Angle                     & $\psi$              \\
                         &                                 & Flow Eccentricity                  & $\varepsilon$       \\ \cmidrule(lr){2-2}
                         & \multirow{2}{*}{Yield Function} & Second Stress Invariant Ratio      & $K_c$                   \\
                         &                                 & Initial Equibiaxial Stress Ratio   & $f_0$ \\ \cmidrule(lr){2-2}
                         & \multirow{3}{*}{Hardening Rule} & Initial Compressive Yield Strength & $\sigma_c^{iy}$     \\
                         &                                 & Peak Compressive Yield Strength    & $\sigma_c^{p}$      \\
                         &                                 & Strain at Peak Compressive Yield   & $\epsilon_c^{p}$   \\ \cmidrule(r){1-2}
\multirow{2}{*}{Damage}  & Compressive                     & Compressive Damage Scaling Factor  & $d_c$               \\ \cmidrule(lr){2-2}
                         & Tensile                         & Tensile Damage Scaling Factor      & $d_t$               \\ \bottomrule
\end{tabularx}
\end{table}